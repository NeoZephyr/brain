MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁

## 两阶段锁协议

![[Pasted image 20230305152837.png]]

事务 A 持有的两个记录的行锁，都是在 commit 的时候才释放的。事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行

在 Innodb 存储引擎中，行锁在需要的时候加上，但要等到事务结束时才释放。因此，如果事务中需要锁定多个行，要把可能造成锁冲突、最可能影响并发度的锁尽量往后放

## 死锁检测

![[Pasted image 20230305152857.png]]

事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有以下两种策略：

1.  出现死锁状态时，直接进入等待，直到超时。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置

```sql
-- 默认值为 50s
show variables like "%innodb_lock_wait_timeout%";
```

2.  发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑

```sql
-- 默认值为 on
show variables like "%innodb_deadlock_detect%";
```

开启死锁检测虽然能够快速发现死锁并进行处理，但是会增加额外负担。当一个事务被锁的时候，就需要查看它所依赖的线程有没有被锁住，如果同时操作同一行的并发线程较多，那么死锁检测就要消耗大量 cpu 资源。这样看上去 cpu 利用率很高，但 tps 却不高

如果要解决这个问题，就要控制并发度，如果同一行记录只有少量的线程在更新，那么死锁检测的成本很低，就不会出现 cpu 利用率很高，但 tps 不高的情况了。而控制并发度，通常有以下方法：

1.  客户端做并发控制，减少客户端的并发线程。缺点是客户端比较多，即使每个客户端只有少量线程，汇总到数据库服务之后，峰值并发数仍会很高
2.  中间件，对于向同行的更新，在进入存储引擎之前进行排队
3.  考虑在业务上，将一行的逻辑改为多行来减少锁冲突

删除一个表里面的前 10000 行数据

方式一：单个语句执行占用时间长，锁的时间也比较长，而且大事务还会导致主从延迟

```sql
delete from T limit 10000;
```

方式二

```sql
-- 循环执行 20 次
delete from T limit 500;
```

方式三：产生锁冲突

```sql
-- 在 20 个连接中同时执行
delete from T limit 500;
```

记录锁：针对单个行记录添加锁
间隙锁：锁住一个范围（索引之间的空隙），但不包括记录本身。采用间隙锁的方式可以防止幻读情况的产生
Next-Key 锁：锁住一个范围，同时锁定记录本身，相当于间隙锁 + 记录锁，可以解决幻读的问题