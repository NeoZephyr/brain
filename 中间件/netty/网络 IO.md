## 读数据

![[网络 IO 读]]

### 流程

当网络数据帧通到达网卡时，网卡会将网络数据帧通过 DMA 的方式放到环形缓冲区 RingBuffer 中

> RingBuffer 是网卡在启动的时候分配和初始化的环形缓冲队列。当 RingBuffer 满的时候，新来的数据包就会被丢弃。可以通过 ifconfig 命令查看网卡收发数据包的情况。其中 overruns 数据项表示当 RingBuffer 满时，被丢弃的数据包。如果发现出现丢包情况，可以通过 ethtool 命令来增大 RingBuffer 长度

DMA 操作完成时，网卡会向 CPU 发起一个硬中断，告诉 CPU 有网络数据到达。CPU 调用网卡驱动注册的硬中断响应程序。网卡硬中断响应程序会为网络数据帧创建内核数据结构 sk_buffer，并将网络数据帧拷贝到 sk_buffer 中。然后发起软中断请求，通知内核有新的网络数据帧到达

> sk_buffer 缓冲区，是一个维护网络帧结构的双向链表，链表中的每一个元素都是一个网络帧。虽然 TCP/IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而无需进行数据复制

内核线程 ksoftirqd 发现有软中断请求到来，随后调用网卡驱动注册的 poll 函数，poll函数将 sk_buffer 中的网络数据包送到内核协议栈中注册的 ip_rcv 函数中。在 ip_rcv 函数中，取出数据包的 IP 头，判断该数据包下一跳的走向，如果数据包是发送给本机的，则取出传输层的协议类型，并去掉数据包的 IP 头，将数据包交给传输层处理

> 每个 CPU 会绑定一个 ksoftirqd 内核线程专门用来处理软中断响应。2 个 CPU 时，就会有 ksoftirqd/0 和 ksoftirqd/1 这两个内核线程
> 
> 网卡接收到数据后，当 DMA 拷贝完成时，向 CPU 发出硬中断，这时哪个 CPU 上响应了这个硬中断，那么在网卡硬中断响应程序中发出的软中断请求也会在这个 CPU 绑定的 ksoftirqd 线程中响应。所以如果发现 Linux 软中断，CPU 消耗都集中在一个核上的话，那么就需要调整硬中断的 CPU 亲和性，来将硬中断打散到不通的 CPU 核上去

当采用的是 TCP 协议时，数据包到达传输层时，会在内核协议栈中的 tcp_rcv 函数处理，在 tcp_rcv 函数中去掉 TCP 头，根据四元组（源IP，源端口，目的IP，目的端口）查找对应的 Socket，如果找到对应的 Socket 则将网络数据包中的传输数据拷贝到 Socket 中的接收缓冲区中。如果没有找到，则发送一个目标不可达的 icmp 包

当程序通过系统调用 read 读取 Socket 接收缓冲区中的数据时，如果接收缓冲区中没有数据，那么应用程序就会在系统调用上阻塞，直到 Socket 接收缓冲区有数据，然后 CPU 将内核空间（Socket 接收缓冲区）的数据拷贝到用户空间，最后系统调用 read 返回，应用程序读取数据

### 性能开销

- 应用程序通过系统调用从用户态转为内核态的开销以及系统调用返回时从内核态转为用户态的开销
- 网络数据从内核空间通过 CPU 拷贝到用户空间的开销
- 内核线程 ksoftirqd 响应软中断的开销
- CPU 响应硬中断的开销
- DMA 拷贝网络数据包到内存中的开销

## 写数据

![[网络 IO 写]]

### 流程

当通过 send 系统调用发送数据时，线程会发生一次用户态到内核态的转换，在内核中首先根据 fd 将真正的 Socket 找出，然后构造 struct msghdr 对象，将用户需要发送的数据全部封装在这个结构体中

调用内核协议栈函数 inet_sendmsg，发送流程进入内核协议栈处理。在进入到内核协议栈之后，内核会找到 Socket 上的具体协议的发送函数。在 TCP 协议的发送函数 tcp_sendmsg 中，创建内核数据结构 sk_buffer，将 struct msghdr 结构体中的发送数据拷贝到 sk_buffer 中。然后将新创建的 sk_buffer 添加到 Socket 发送队列的尾部

> Socket 的发送队列是由 sk_buffer 组成的一个双向链表

虽然发送数据已经拷贝到了内核 Socket 中的发送队列中，但由于 TCP 协议的流量控制和拥塞控制，数据包并不一定会立马被发送出去，需要符合 TCP 协议的发送条件。如果没有达到发送条件，那么本次 send 系统调用就会直接返回。如果符合发送条件，则开始调用 tcp_write_xmit 内核函数。在这个函数中，会循环获取 Socket 发送队列中待发送的 sk_buffer，然后进行拥塞控制以及滑动窗口的管理

将从 Socket 发送队列中获取到的 sk_buffer 重新拷贝一份，设置 sk_buffer 副本中的 TCP 头。在设置 TCP 头的时候，只是把指针指向 sk_buffer 的合适位置。后面再设置 IP 头的时候，在把指针移动一下就行，避免频繁的内存申请和拷贝，效率很高

> 为什么不直接使用 Socket 发送队列中的 sk_buffer 而是需要拷贝一份呢？因为 TCP 协议是支持丢包重传的，在没有收到对端的 ACK 之前，这个 sk_buffer 是不能删除的。内核每次调用网卡发送数据的时候，实际上传递的是 sk_buffer 的拷贝副本，当网卡把数据发送出去后，sk_buffer 拷贝副本会被释放。当收到对端的 ACK 之后，Socket 发送队列中的 sk_buffer 才会被真正删除

当设置完 TCP 头后，内核协议栈传输层的事情就做完了，下面通过调用 ip_queue_xmit 内核函数，正式来到内核协议栈网络层的处理。将 sk_buffer 中的指针移动到 IP 头位置上，设置 IP 头。执行 netfilters 过滤。过滤通过之后，如果数据大于 MTU 的话，则执行分片。检查 Socket 中是否有缓存路由表，如果没有的话，则查找路由项，并缓存到 Socket 中。接着在把路由表设置到 sk_buffer 中

内核协议栈网络层的事情处理完后，现在发送流程进入了到了邻居子系统，邻居子系统位于内核协议栈中的网络层和网络接口层之间，用于发送 ARP 请求获取 MAC 地址，然后将 sk_buffer 中的指针移动到 MAC 头位置，填充 MAC 头

经过邻居子系统的处理，现在 sk_buffer 中已经封装了一个完整的数据帧，随后内核将 sk_buffer 交给网络设备子系统进行处理。网络设备子系统主要做以下几项事情：

- 选择发送队列（RingBuffer）。因为网卡拥有多个发送队列，所以在发送前需要选择一个发送队列。将 sk_buffer 添加到发送队列中
- 循环从发送队列（RingBuffer）中取出 sk_buffer，调用内核函数 sch_direct_xmit 发送数据，其中会调用网卡驱动程序来发送数据

> 以上过程全部是用户线程的内核态在执行，占用的 CPU 时间是系统态时间，当分配给用户线程的 CPU quota 用完的时候，会触发 NET_TX_SOFTIRQ 类型的软中断，内核线程 ksoftirqd 会响应这个软中断，并执行 NET_TX_SOFTIRQ 类型的软中断注册的回调函数 net_tx_action，在回调函数中会执行到驱动程序函数 dev_hard_start_xmit 来发送数据

无论是用户线程的内核态还是触发 NET_TX_SOFTIRQ 类型的软中断，在发送数据的时候最终会调用到网卡的驱动程序函数 dev_hard_start_xmit 来发送数据。在网卡驱动程序函数 dev_hard_start_xmit 中会将 sk_buffer 映射到网卡可访问的内存 DMA 区域，最终网卡驱动程序通过 DMA 的方式将数据帧通过物理网卡发送出去

数据发送完毕后，网卡设备会向 CPU 发送一个硬中断，CPU 调用网卡驱动程序注册的硬中断响应程序，在硬中断响应中触发 NET_RX_SOFTIRQ 类型的软中断，在软中断的回调函数 igb_poll 中清理释放 sk_buffer，清理网卡发送队列，解除 DMA 映射

### 性能开销

- 应用程序在调用系统调用 send 的时候会从用户态转为内核态以及发送完数据后，系统调用返回时从内核态转为用户态的开销
- 用户线程内核态 CPU quota 用尽时触发 NET_TX_SOFTIRQ 类型软中断，内核响应软中断的开销
- 网卡发送完数据，向 CPU 发送硬中断，CPU 响应硬中断的开销。以及在硬中断中发送 NET_RX_SOFTIRQ 软中断执行具体的内存清理动作。内核响应软中断的开销
- 内存拷贝的开销
	- TCP 协议对应的发送函数 tcp_sendmsg 会申请 sk_buffer，将用户要发送的数据拷贝到 sk_buffer 中
	- 传输层到网络层的时候，会拷贝一个 sk_buffer 副本出来，将这个 sk_buffer 副本向下传递。原始 sk_buffer 保留在 Socket 发送队列中，等待网络对端 ACK，对端 ACK 后删除 Socket 发送队列中的 sk_buffer。对端没有发送 ACK，则重新从 Socket 发送队列中发送，实现 TCP 协议的可靠传输
	- 在网络层，如果发现要发送的数据大于 MTU，则会进行分片操作，申请额外的 sk_buffer，并将原来的 sk_buffer 拷贝到多个小的 sk_buffer 中

## IO 模型

可以将网络数据包接收流程总结为两个阶段

![[IO 两阶段]]

数据准备阶段：在这个阶段，网络数据包到达网卡，通过 DMA 的方式将数据包拷贝到内存中，然后经过硬中断，软中断，接着通过内核线程 ksoftirqd 经过内核协议栈的处理，最终将数据发送到内核 Socket 的接收缓冲区中

数据拷贝阶段：当数据到达内核 Socket 的接收缓冲区中时，此时数据存在于内核空间中，需要将数据拷贝到用户空间中，才能够被应用程序读取

当应用程序发起系统调用 read 时，线程从用户态转为内核态，读取内核 Socket 的接收缓冲区中的网络数据

### 阻塞与非阻塞

#### 阻塞
![[阻塞 IO]]

如果内核 Socket 的接收缓冲区没有数据，线程就会一直等待，直到 Socket 接收缓冲区有数据为止。随后将数据从内核空间拷贝到用户空间，系统调用 read 返回

#### 非阻塞
![[非阻塞 IO]]

阻塞和非阻塞主要的区分是在第一阶段：数据准备阶段。当 Socket 的接收缓冲区中没有数据的时候，阻塞模式下应用线程会一直等待。非阻塞模式下应用线程不会等待，系统调用直接返回错误标志 EWOULDBLOCK。当 Socket 的接收缓冲区中有数据的时候，阻塞和非阻塞的表现是一样的，都会进入第二阶段等待数据从内核空间拷贝到用户空间，然后系统调用返回

### 同步与异步

#### 同步

同步模式在数据准备好后，是由用户线程的内核态来执行第二阶段。所以应用程序会在第二阶段发生阻塞，直到数据从内核空间拷贝到用户空间，系统调用才会返回

#### 异步

异步模式下是由内核来执行第二阶段的数据拷贝操作，当内核执行完第二阶段，会通知用户线程 IO 操作已经完成，并将数据回调给用户线程。所以在异步模式下 数据准备阶段和数据拷贝阶段均是由内核来完成，不会对应用程序造成任何阻塞

基于以上特征，可以看到异步模式需要内核的支持，比较依赖操作系统底层的支持